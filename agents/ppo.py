import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim
from agents.model_ppo import Gaussian
import random
from agents.utils import soft_update


class PPO():
    """Interacts with and learns from the environment."""
    
    def __init__(self, network, device, logger,
                 LR=1e-2,
                 WEIGHT_DECAY=1.e-4,
                 GRADIENT_CLIP=1, 
                 EPOCHS=4, 
                 BATCH_SIZE=32,
                GAMMA=0.99,
                GAE_TAU=0.95,
                CLIP_EPSILON=1e-1,
                C1=0.5,
                C2=0.01
                ):
        self.device = device
        self.network = network
        self.optim = optim.Adam(self.network.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
        self.GRADIENT_CLIP = GRADIENT_CLIP
        self.EPOCHS=EPOCHS
        self.BATCH_SIZE=BATCH_SIZE
        self.GAMMA=GAMMA
        self.CLIP_EPSILON=CLIP_EPSILON
        self.GAE_TAU=GAE_TAU
        self.C1=C1
        self.C2=C2
        self.loss_function = lambda estimated, actual: (estimated - actual)**2
#         self.loss_function = torch.nn.SmoothL1Loss()
#         self.loss_function = F.mse_loss
        self.logger = logger
        self.steps = 0
        self.steps_backprop = 0
    
    def save(self, filename):
        torch.save(self.network.state_dict(),"weights/" + filename)
     
    def load(self, path):
        self.network.load_state_dict(torch.load(path))
    
    def act(self, states):
        """ 
        """
        with torch.no_grad():
            states = torch.tensor(states).float().to(self.device)
            actions, log_probs = self.network(states)
        return actions.cpu().detach().numpy(), log_probs.cpu().detach().numpy()
    
    def estimate(self, full_states):
        """ 
        """
        with torch.no_grad():
            full_states = torch.tensor(full_states).float().to(self.device)
            ret = self.network.estimate(full_states)
        return ret.cpu().detach().numpy()
    
    def learn(self, states, actions, log_probs, values, rewards, next_states, dones, full_states, next_values):
        """
        Noise Reduction: Collect more trajectories and when computing the gradients, use the mean gradient accross all trajectories
        Credit Assignment: 
            instead of using the total sum of the reward, use the cumulated rewards
            gradient = SUM[ SUM(advantage*Gradient(log_prob(a|s) )]
        Importance Sampling -> Surrogate:
            Network theta -> generate action a for log_prob
            Network theta'-> find out how likely is the same action a to be generated by theta'
            Instead of gradient(log_prob(a|s)) use gradient(log_prob' - log_prob)*exp() = gradient(prob'/prob)
                This is about re-weighting factor
            Need to understand Importance Sampling and the how to apply it to probabilities
        Clip Surrogate
            apply a gradient which is always less than 1+epsilon of the ration
        """
        states = torch.tensor(states, device=self.device, dtype=torch.float32, requires_grad=False)
        actions = torch.tensor(actions, device=self.device, dtype=torch.float32, requires_grad=False)
        log_probs = torch.tensor(log_probs, device=self.device, dtype=torch.float32, requires_grad=False)
        values = torch.tensor(values, device=self.device, dtype=torch.float32, requires_grad=False)
        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32, requires_grad=False)
        next_states = torch.tensor(next_states, device=self.device, dtype=torch.float32, requires_grad=False)
        dones = 1 - torch.tensor(dones, device=self.device, dtype=torch.float32, requires_grad=False)
        full_states = torch.tensor(full_states, device=self.device, dtype=torch.float32, requires_grad=False)
        next_values = torch.tensor(next_values, device=self.device, dtype=torch.float32, requires_grad=False)
        
        self.steps += 1
        
        self.logger.add_scalar('ppo/max_reward', rewards.max(), self.steps)
        
        returns = next_values[-1]

        advantages = [None] * actions.shape[0]
        returns_array = [None] * actions.shape[0]
        advantage = torch.zeros(rewards[0].shape, device=self.device, dtype=torch.float32)
        
        assert returns.shape == rewards[-1].shape, "{} != {}".format(returns.shape, rewards[-1].shape)
        assert dones[-1].shape == rewards[-1].shape, "{} != {}".format(dones[-1].shape, rewards[-1].shape)
        assert next_values[-1].shape == dones[-1].shape, "{} != {}".format(next_values[-1].shape, dones[-1].shape)
        assert advantage.shape == dones[-1].shape, "{} != {}".format(advantage.shape, dones[-1].shape)

        for i in reversed(range(states.shape[0])):
            returns = rewards[i] + self.GAMMA*returns*dones[i]
            returns_array[i]=returns.unsqueeze(0).detach()

            # according to the implementation of ShangTong and to the paper 
            # High Dimensional Continuous Control Using Generalized Advantage Estimation from arxiv
            td_error = rewards[i] + self.GAMMA * dones[i] * next_values[i] - values[i]
            assert advantage.shape == td_error.shape, "{} != {}".format(advantage.shape, td_error.shape)
            
            advantage = advantage * self.GAE_TAU * self.GAMMA * dones[i] + td_error
            advantages[i] = advantage.unsqueeze(0).detach()

        returns_array = torch.cat(returns_array, dim=0)
        advantages = torch.cat(advantages, dim=0).unsqueeze(-1)
        advantages = (advantages - advantages.mean()) / advantages.std()
        
        self.logger.add_scalar('ppo/returns', returns_array.mean(), self.steps)
        self.logger.add_scalar('ppo/advantages', returns_array.mean(), self.steps)
        
        # prepare states, actions, returns,
        for epoch in range(self.EPOCHS):
            # Shuffle the indices
            for indices in self.batch_indices(len(states), self.BATCH_SIZE):
                idx = torch.tensor(indices).long()
                sampled_states = states[idx]
                sampled_full_states = full_states[idx]
                sampled_actions = actions[idx]         
                sampled_log_probs = log_probs[idx].detach()
                sampled_advantages = advantages[idx]
                sampled_returns = returns_array[idx].detach()
                
                # find out how likely the new network would have chosen the sampled_actions at the given sampled_states
                new_log_probs = self.network.get_log_probs(sampled_actions, sampled_states)
            
                assert new_log_probs.shape == sampled_log_probs.shape, "{} != {}".format(new_log_probs.shape, sampled_log_probs.shape)
                assert sampled_advantages.shape == sampled_log_probs.shape, "{} != {}".format(sampled_advantages.shape, sampled_log_probs.shape)
                ratio = (new_log_probs - sampled_log_probs).exp()
                clip = torch.clamp(ratio, 1-self.CLIP_EPSILON, 1+self.CLIP_EPSILON)
                clipped_surrogate = torch.min(ratio*sampled_advantages, clip*sampled_advantages)
                
                # This entropy is included in the calculation of the gradient based on
                # Udacity notebook of Pong-PPO under the chapter of Policy Gradient
                # https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#ppo
                # the paper, https://arxiv.org/pdf/1707.06347.pdf
                # and arxiv insight https://www.youtube.com/watch?v=5P7I-xPq8u8
                # to promote exploration
                entropy = -(new_log_probs.exp()*sampled_log_probs) 
                
                estimated_values = self.network.estimate(sampled_full_states).squeeze(-1)
                
                assert estimated_values.shape == sampled_returns.shape, "{} != {}".format(estimated_values
.shape, sampled_returns.shape)
                value_loss = self.loss_function(estimated_values, sampled_returns)
                
                self.optim.zero_grad()
                value_loss = value_loss.unsqueeze(-1)
                assert clipped_surrogate.shape == value_loss.shape, "{} != {}".format(clipped_surrogate.shape, value_loss.shape)
                assert entropy.shape == value_loss.shape, "{} != {}".format(entropy.shape, value_loss.shape)
                total_loss = - (clipped_surrogate - self.C1 * value_loss + self.C2 * entropy).mean()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.GRADIENT_CLIP)
                self.optim.step()
                
                self.steps_backprop += 1
                self.logger.add_scalar('ppo/total_loss', total_loss, self.steps_backprop)
                
                del idx, sampled_states, sampled_actions
                del sampled_log_probs, sampled_advantages, sampled_returns
                del new_log_probs, ratio, clip, clipped_surrogate, estimated_values, value_loss, entropy

        del states, actions, log_probs, values, rewards, next_states, dones, returns, advantages, returns_array
    
    def batch_indices(self, length, batch_size):
        indices = np.arange(length)
        np.random.shuffle(indices)
        for i in range(1 + length // batch_size):
            start = batch_size*i
            end = start + batch_size
            end = min(length, end)
            if start >= length:
                return
            yield indices[start:end]